hydra:
  run:
    dir: .
  output_subdir: null

# Which action to perform: "preprocess" or "train"
mode: train

dataset:
  name: databricks/databricks-dolly-15k
  split: train
  subset: null
  val_subset: null
  val_split_ratio: 0.05

formatter:
  _target_: training.formatting.InstructionFormatter
  instruction_field: instruction
  input_field: input
  target_field: output
  template: null

model:
  name: gpt2
  max_length: 512

lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules:
    - q_proj
    - v_proj

preprocess:
  output_dir: data
  train:
    num_shards: 4
    subset: null
    max_examples: null
  val:
    num_shards: 1
    subset: null
    max_examples: null
  val_split_ratio: 0.05

train:
  data_dir: data
  train_shard_template: shard_{rank}
  val_shard_template: null
  trainer_class: training.train.sft_trainer.SFTTrainer
  model_builder: null
  optimizer_builder: null
  scheduler_builder: null
  metrics_logger: null
  batch_size: 4
  val_batch_size: null
  num_epochs: 1
  lr: 5e-5
  weight_decay: 0.01
  warmup_ratio: 0.03
  output_dir: outputs/dolly_sft
  use_lora: false
  metrics_port: 8000
  max_steps: null
  deepspeed_config_path: null
  log_every_n_steps: 1
